import os
import time
import torch
import httpx
import pandas as pd
import matplotlib.pyplot as plt
import re
from transformers import AutoTokenizer, AutoModel
from huggingface_hub import login
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_mistralai import MistralAIEmbeddings, ChatMistralAI
from credentials.key import key_Mistral, HF_TOKEN
from parametres.params import parent_dir, input_dir, output_dir, database_dir, RESF_dir, docs_dir, tables_dir, txt_dir, graph_dir, report_dir
from mistralai import Mistral

print("üöÄ D√©marrage du programme...")

# ============================ Initialisation des cl√©s API ============================
os.environ["HF_TOKEN"] = HF_TOKEN
login(os.environ["HF_TOKEN"])

# ============================ Initialisation des clients ============================
print("üîë Initialisation du client Mistral...")
client = Mistral(api_key=key_Mistral)

print("ü§ñ Initialisation du mod√®le de chat Mistral...")
try:
    model_chat = ChatMistralAI(model="mistral-large-latest", api_key=key_Mistral)
    print("‚úÖ Mod√®le de chat Mistral initialis√© avec succ√®s")
except Exception as e:
    print(f"‚ùå Erreur lors de l'initialisation du mod√®le Mistral: {e}")
    exit()

# ============================ Mod√®le d'embeddings local ============================
model = AutoModel.from_pretrained("BAAI/bge-m3")
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")

def get_embeddings(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    with torch.no_grad():
        embeddings = model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings

# ============================ Prompt Template ============================
PROMPT_TEMPLATE = """
Answer the question based only on the following context:
{context}
Answer the question based on the above context: {question} in French.
Provide a structured table of figures.
Don't justify your answers.
Do not say \"according to the context\" or \"mentioned in the context\" or similar.
"""
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

# ============================ R√©sum√© par chapitre ============================
path_input = os.path.join(parent_dir, input_dir)
path_resf = os.path.join(path_input, RESF_dir)
path_output = os.path.join(parent_dir, output_dir)
path_graph = os.path.join(path_output, graph_dir)
path_database = os.path.join(path_output, database_dir)
path_txt = os.path.join(path_output, txt_dir)
path_report = os.path.join(path_output, report_dir)

os.makedirs(path_input, exist_ok=True)
os.makedirs(path_output, exist_ok=True)
os.makedirs(path_graph, exist_ok=True)
os.makedirs(path_database, exist_ok=True)
os.makedirs(path_txt, exist_ok=True)
os.makedirs(path_report, exist_ok=True)

# ============================ Fonction de traitement OCR ============================
def process_ocr(annee):
    input_file = f"RESF_{annee}.pdf"
    input_path = os.path.join(path_resf, input_file)
    
    if not os.path.exists(input_path):
        print(f"‚ö†Ô∏è Fichier {input_file} non trouv√© dans {input_path}")
        return None

    print(f"üìÑ Traitement du fichier {input_file}...")
    
    try:
        uploaded_pdf = client.files.upload(
            file={
                "file_name": input_path,
                "content": open(input_path, "rb"),
            },
            purpose="ocr"
        )

        signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)

        ocr_response = client.ocr.process(
            model="mistral-ocr-latest",
            document={
                "type": "document_url",
                "document_url": signed_url.url,
            }
        )

        ocr_text_with_pages = ""
        for page_number, page in enumerate(ocr_response.pages, start=1):
            if page.markdown:
                ocr_text_with_pages += f"[{page_number}]\n{page.markdown}\n"

        file_name = f"RESF_{annee}.txt"
        file_path = os.path.join(path_txt, file_name)
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(ocr_text_with_pages)
        
        print(f"‚úÖ OCR termin√© pour {input_file}")
        return file_path
        
    except Exception as e:
        print(f"‚ùå Erreur lors du traitement OCR de {input_file}: {str(e)}")
        return None
    
def reponse_question(question, filename):
    print(f"\nüìÑ Traitement du fichier: {filename}")
    file_path = os.path.join(path_txt, filename)
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    document = Document(page_content=content, metadata={"source": filename})
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=25)
    chunks = text_splitter.split_documents([document])
    
    chroma_path = f"{path_database}/db_{filename.replace('.txt', '')}"
    print("üîç Initialisation des embeddings Mistral...")
    embeddings = MistralAIEmbeddings(model="mistral-embed", api_key=key_Mistral)

    if not os.path.exists(chroma_path):
        print("üìö Cr√©ation de la base de donn√©es Chroma...")
        db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=chroma_path)
    else:
        print("üìö Chargement de la base de donn√©es Chroma existante...")
        db_chroma = Chroma(persist_directory=chroma_path, embedding_function=embeddings)
    
    print("üîç Recherche de documents similaires...")
    docs_chroma = db_chroma.similarity_search_with_score(question, k=5)
    context_text = "\n\n".join([doc.page_content for doc, _score in docs_chroma])

    prompt = prompt_template.format(context=context_text, question=question)
    print("ü§ñ G√©n√©ration de la r√©ponse avec Mistral...")
    response_text = invoke_with_retry(model_chat, prompt)

    # Cr√©ation du nom du fichier de rapport bas√© sur le fichier source
    report_filename = f"rapport_{filename.replace('.txt', '.txt')}"
    report_file_path = os.path.join(path_report, report_filename)
    
    with open(report_file_path, "w", encoding="utf-8") as f:
        f.write(response_text.content)

    print(f"‚úÖ R√©ponse g√©n√©r√©e pour {filename} ‚Üí {report_file_path}")
    return response_text.content

# ============================ Requ√™te avec gestion des erreurs ============================
def invoke_with_retry(model, prompt, max_retries=10):
    retries = 0
    while retries < max_retries:
        try:
            print(f"üì§ Envoi de la requ√™te √† l'API Mistral (tentative {retries + 1})...")
            response = model.invoke(prompt)
            print("‚úÖ R√©ponse re√ßue de l'API Mistral")
            return response
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                wait_time = 2 ** retries
                print(f"‚ö†Ô∏è Rate limit exceeded. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
                retries += 1
            else:
                print(f"‚ùå Erreur HTTP: {e}")
                raise e
    raise Exception("‚ùå Max retries reached, request failed.")

def extraire_numeriques(texte):
    """
    Extrait les valeurs num√©riques et leurs labels du texte
    """
    # Pattern pour trouver les nombres avec leurs labels
    pattern = r'([^:]+):\s*([-+]?\d*\.?\d+)'
    matches = re.findall(pattern, texte)
    
    resultats = {}
    for label, valeur in matches:
        label = label.strip().lower()
        try:
            valeur = float(valeur)
            resultats[label] = valeur
        except ValueError:
            continue
    return resultats

def generer_graphiques(reponses, question):
    """
    G√©n√®re des graphiques √† partir des r√©ponses pour chaque ann√©e
    """
    # Cr√©er un DataFrame pour stocker les donn√©es
    data = {}
    
    for annee, reponse in reponses.items():
        valeurs = extraire_numeriques(reponse)
        if valeurs:
            data[annee] = valeurs
    
    if not data:
        print("‚ö†Ô∏è Aucune donn√©e num√©rique trouv√©e pour g√©n√©rer des graphiques")
        return
    
    # Cr√©er un DataFrame
    df = pd.DataFrame(data).T
    
    # Cr√©er un dossier pour les graphiques s'il n'existe pas
    os.makedirs(path_graph, exist_ok=True)
    
    # G√©n√©rer diff√©rents types de graphiques
    # 1. Graphique en barres pour chaque indicateur
    for colonne in df.columns:
        plt.figure(figsize=(10, 6))
        df[colonne].plot(kind='bar')
        plt.title(f'√âvolution de {colonne} par ann√©e')
        plt.xlabel('Ann√©e')
        plt.ylabel('Valeur')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # Sauvegarder le graphique
        graph_filename = f"{colonne.replace(' ', '_')}_evolution.png"
        graph_path = os.path.join(path_graph, graph_filename)
        plt.savefig(graph_path)
        plt.close()
        print(f"‚úÖ Graphique sauvegard√© : {graph_path}")
    
    # 2. Graphique en ligne pour tous les indicateurs
    plt.figure(figsize=(12, 6))
    for colonne in df.columns:
        plt.plot(df.index, df[colonne], marker='o', label=colonne)
    
    plt.title('√âvolution des indicateurs par ann√©e')
    plt.xlabel('Ann√©e')
    plt.ylabel('Valeur')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    
    # Sauvegarder le graphique combin√©
    graph_filename = "evolution_globale.png"
    graph_path = os.path.join(path_graph, graph_filename)
    plt.savefig(graph_path)
    plt.close()
    print(f"‚úÖ Graphique global sauvegard√© : {graph_path}")

def comparer_reponses(question, annees, generer_graphiques_option=False):
    """
    Compare les r√©ponses pour plusieurs ann√©es et g√©n√®re un rapport comparatif
    """
    print(f"\nüîÑ Comparaison des r√©ponses pour les ann√©es {annees}")
    
    # Collecter les r√©ponses pour chaque ann√©e
    reponses = {}
    for annee in annees:
        filename = f"RESF_{annee}.txt"
        if os.path.exists(os.path.join(path_txt, filename)):
            reponse = reponse_question(question, filename)
            reponses[annee] = reponse
        else:
            print(f"‚ö†Ô∏è Fichier {filename} non trouv√©")
    
    # G√©n√©rer un rapport comparatif
    if reponses:
        rapport_comparatif = f"Rapport comparatif pour la question : {question}\n\n"
        for annee, reponse in reponses.items():
            rapport_comparatif += f"\n=== Ann√©e {annee} ===\n{reponse}\n"
        
        # Sauvegarder le rapport comparatif
        rapport_filename = f"rapport_comparatif_{'-'.join(annees)}.txt"
        rapport_path = os.path.join(path_report, rapport_filename)
        with open(rapport_path, "w", encoding="utf-8") as f:
            f.write(rapport_comparatif)
        print(f"‚úÖ Rapport comparatif g√©n√©r√© ‚Üí {rapport_path}")
        
        # G√©n√©rer les graphiques si demand√©
        if generer_graphiques_option:
            generer_graphiques(reponses, question)

def main():
    annees = ["2020", "2021", "2022"]
    question = "Donne moi les chiffres d'inflation, les chiffres de d√©ficit et de croissance de la France en pourcentage du PIB dans les RESF 2020 et 2021 pour toutes les ann√©es disponibles"
    
    for annee in annees:
        print(f"\nTraitement de l'ann√©e {annee}...")
        
        # √âtape 1: OCR
        input_path = process_ocr(annee)
        if input_path is None:
            continue
    
    # √âtape 2: G√©n√©ration de l'analyse comparative
    print("\nVoulez-vous g√©n√©rer des graphiques pour visualiser les donn√©es ? (oui/non)")
    reponse = input().lower().strip()
    generer_graphiques_option = reponse in ['oui', 'o', 'yes', 'y']
    
    comparer_reponses(question, annees, generer_graphiques_option)

if __name__ == "__main__":
    main()